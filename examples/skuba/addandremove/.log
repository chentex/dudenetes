[1;37mFeature:[0m Bootstraping
  Bootstrapping the cluster is the initial process of starting up the cluster
  and defining which of the nodes are masters and which workers.
  For maximum automation of this process SUSE CaaS Platform uses the skuba package.
  
  The tests assumes you have skuba already available in your machine
  and you have already deployed the requred infrastructure along with
  the SSH-agent running from the terminal you are issuing dudenetes commands.

  [1;37mScenario:[0m Initialize the cluster                                                                                   [1;30m# addandremove/addAndRemove.feature:11[0m
    [32mGiven[0m [32mthere a deployed infrastructure for [0m[1;32m1[0m[32m lb, [0m[1;32m3[0m[32m masters and [0m[1;32m3[0m[32m workers[0m                                          [1;30m# boostraping_test.go:14 -> thereADeployedInfrastructureForLbMastersAndWorkers[0m
    [32mWhen[0m [32myou do "[0m[1;32mskuba cluster init --control-plane $loadbalancer my-cluster[0m[32m"[0m                                        [1;30m# boostraping_test.go:30 -> youDo[0m
    [31mThen[0m [31m"[0m[1;31mmy-cluster[0m[31m" dir should be created containing the IP of the loadbalancer "[0m[1;31mgrep -r $loadbalancer my-cluster[0m[31m"[0m [1;30m# boostraping_test.go:39 -> dirShouldBeCreatedContainingTheIPOfTheLoadbalancer[0m
    [1;31m
	Failed:
	-------
	grep -r 10.84.153.134 my-cluster

	Output:
	-------
	exit status 1: 

	Error:
	-------
	exit status 1[0m
[1;37mFeature:[0m Bootstraping
  Bootstrapping the cluster is the initial process of starting up the cluster
  and defining which of the nodes are masters and which workers.
  For maximum automation of this process SUSE CaaS Platform uses the skuba package.
  
  The tests assumes you have skuba already available in your machine
  and you have already deployed the requred infrastructure along with
  the SSH-agent running from the terminal you are issuing dudenetes commands.

  [1;37mScenario:[0m Initialize the cluster                                                                                   [1;30m# addandremove/addAndRemove.feature:11[0m
    [32mGiven[0m [32mthere a deployed infrastructure for [0m[1;32m1[0m[32m lb, [0m[1;32m3[0m[32m masters and [0m[1;32m3[0m[32m workers[0m                                          [1;30m# boostraping_test.go:14 -> thereADeployedInfrastructureForLbMastersAndWorkers[0m
    [32mWhen[0m [32myou do "[0m[1;32mskuba cluster init --control-plane $loadbalancer my-cluster[0m[32m"[0m                                        [1;30m# boostraping_test.go:30 -> youDo[0m
    [32mThen[0m [32m"[0m[1;32mmy-cluster[0m[32m" dir should be created containing the IP of the loadbalancer "[0m[1;32mgrep -r $loadbalancer my-cluster[0m[32m"[0m [1;30m# boostraping_test.go:39 -> dirShouldBeCreatedContainingTheIPOfTheLoadbalancer[0m

  [1;37mScenario:[0m Bootstrap the master node                                                                                                         [1;30m# addandremove/addAndRemove.feature:16[0m
    [32mGiven[0m [32myou run "[0m[1;32mskuba -v 5 node bootstrap --user sles --sudo --target $master1 master-1[0m[32m" with a timeout of [0m[1;32m500[0m[32m seconds[0m                     [1;30m# boostraping_test.go:49 -> youRunWithATimeoutOfSeconds[0m
    [32mAnd[0m [32mafter configuring your new kubeconfig into this "[0m[1;32mcp admin.conf $HOME/.kube/config[0m[32m"[0m                                                    [1;30m# boostraping_test.go:58 -> afterConfiguringYourNewKubeconfigIntoThis[0m
    [32mThen[0m [32mthe master must be ready within [0m[1;32m500[0m[32m seconds timeout "[0m[1;32mkubectl get nodes |  grep master-1 | grep --invert-match NotReady | grep Ready[0m[32m"[0m [1;30m# boostraping_test.go:67 -> theMasterMustBeReadyWithinSecondsTimeout[0m

  [1;37mScenario:[0m Join 1 worker                                                                                                                [1;30m# addandremove/addAndRemove.feature:21[0m
    [32mWhen[0m [32myou run skuba node join "[0m[1;32mskuba -v 5 node join --role worker --user sles --sudo --target $worker1 worker-1[0m[32m" with [0m[1;32m500[0m[32m sec timeout[0m [1;30m# boostraping_test.go:75 -> youRunSkubaNodeJoinWithSecTimeout[0m
    [32mThen[0m [32mthe node should be ready "[0m[1;32mkubectl get nodes | grep worker-1 | grep --invert-match NotReady | grep Ready[0m[32m" within [0m[1;32m500[0m[32m sec[0m         [1;30m# boostraping_test.go:84 -> theNodeShouldBeReadyWithinSec[0m

  [1;37mScenario:[0m Add another master node, remove it, and then add another master node                                                                      [1;30m# addandremove/addAndRemove.feature:25[0m
    [32mGiven[0m [32myou run "[0m[1;32mskuba -v 5 node join --role master --user sles --sudo --target $master2 master-2[0m[32m" with a timeout of [0m[1;32m500[0m[32m seconds[0m                    [1;30m# boostraping_test.go:49 -> youRunWithATimeoutOfSeconds[0m
    [32mThen[0m [32mthe master must be ready within [0m[1;32m500[0m[32m seconds timeout "[0m[1;32mkubectl get nodes |  grep master-2 | grep --invert-match NotReady | grep Ready[0m[32m"[0m         [1;30m# boostraping_test.go:67 -> theMasterMustBeReadyWithinSecondsTimeout[0m
    [33mAnd[0m [33mnow you must have two ready masters "kubectl get nodes |  grep master | grep --invert-match NotReady | grep Ready | wc -l | grep 2"[0m
    [33mWhen[0m [33myou remove this master node "skuba node remove master-2 --drain-timeout 5s"[0m
    [33mThen[0m [33mthere must be only one master at your cluster "kubectl get nodes | grep master | grep --invert-match NotReady | grep Ready | wc -l | grep 1"[0m
    [36mGiven[0m [36myou run "[0m[1;36mskuba -v 5 node join --role master --user sles --sudo --target $master3 master-3[0m[36m" with a timeout of [0m[1;36m500[0m[36m seconds[0m                    [1;30m# boostraping_test.go:49 -> youRunWithATimeoutOfSeconds[0m
    [36mThen[0m [36mthe master must be ready within [0m[1;36m500[0m[36m seconds timeout "[0m[1;36mkubectl get nodes |  grep master-3 | grep --invert-match NotReady | grep Ready[0m[36m"[0m         [1;30m# boostraping_test.go:67 -> theMasterMustBeReadyWithinSecondsTimeout[0m
    [33mAnd[0m [33mnow you must have two ready masters "kubectl get nodes |  grep master | grep --invert-match NotReady | grep Ready | wc -l | grep 2"[0m

  [1;37mScenario:[0m Join another worker node, remove it, and then add another worker node                                                                     [1;30m# addandremove/addAndRemove.feature:35[0m
    [32mWhen[0m [32myou run skuba node join "[0m[1;32mskuba -v 5 node join --role worker --user sles --sudo --target $worker2 worker-2[0m[32m" with [0m[1;32m500[0m[32m sec timeout[0m              [1;30m# boostraping_test.go:75 -> youRunSkubaNodeJoinWithSecTimeout[0m
    [32mThen[0m [32mthe node should be ready "[0m[1;32mkubectl get nodes | grep worker-2 | grep --invert-match NotReady | grep Ready[0m[32m" within [0m[1;32m500[0m[32m sec[0m                      [1;30m# boostraping_test.go:84 -> theNodeShouldBeReadyWithinSec[0m
    [33mAnd[0m [33mnow you must have two ready workers "kubectl get nodes |  grep worker | grep --invert-match NotReady | grep Ready | wc -l | grep 2"[0m
    [33mWhen[0m [33myou remove this worker node "skuba node remove worker-2 --drain-timeout 5s"[0m
    [33mThen[0m [33mthere must be only one worker at your cluster "kubectl get nodes | grep worker | grep --invert-match NotReady | grep Ready | wc -l | grep 1"[0m
    [36mWhen[0m [36myou run skuba node join "[0m[1;36mskuba -v 5 node join --role worker --user sles --sudo --target $worker3 worker-3[0m[36m" with [0m[1;36m500[0m[36m sec timeout[0m              [1;30m# boostraping_test.go:75 -> youRunSkubaNodeJoinWithSecTimeout[0m
    [36mThen[0m [36mthe node should be ready "[0m[1;36mkubectl get nodes | grep worker-3 | grep --invert-match NotReady | grep Ready[0m[36m" within [0m[1;36m500[0m[36m sec[0m                      [1;30m# boostraping_test.go:84 -> theNodeShouldBeReadyWithinSec[0m
    [33mAnd[0m [33mnow you must have two ready workers "kubectl get nodes |  grep worker | grep --invert-match NotReady | grep Ready | wc -l | grep 2"[0m

5 scenarios ([32m3 passed[0m, [33m2 undefined[0m)
24 steps ([32m12 passed[0m, [33m8 undefined[0m, [36m4 skipped[0m)
13m9.431847635s

[33mYou can implement step definitions for undefined steps with these snippets:[0m
[33m
func nowYouMustHaveTwoReadyMasters(arg1 string) error {
	return godog.ErrPending
}

func youRemoveThisMasterNode(arg1 string) error {
	return godog.ErrPending
}

func thereMustBeOnlyOneMasterAtYourCluster(arg1 string) error {
	return godog.ErrPending
}

func nowYouMustHaveTwoReadyWorkers(arg1 string) error {
	return godog.ErrPending
}

func youRemoveThisWorkerNode(arg1 string) error {
	return godog.ErrPending
}

func thereMustBeOnlyOneWorkerAtYourCluster(arg1 string) error {
	return godog.ErrPending
}

func FeatureContext(s *godog.Suite) {
	s.Step(`^now you must have two ready masters "([^"]*)"$`, nowYouMustHaveTwoReadyMasters)
	s.Step(`^you remove this master node "([^"]*)"$`, youRemoveThisMasterNode)
	s.Step(`^there must be only one master at your cluster "([^"]*)"$`, thereMustBeOnlyOneMasterAtYourCluster)
	s.Step(`^now you must have two ready workers "([^"]*)"$`, nowYouMustHaveTwoReadyWorkers)
	s.Step(`^you remove this worker node "([^"]*)"$`, youRemoveThisWorkerNode)
	s.Step(`^there must be only one worker at your cluster "([^"]*)"$`, thereMustBeOnlyOneWorkerAtYourCluster)
}
[0m
